# DataScience_team_project: Прогнозування відтоку клієнтів

Цей проєкт розроблено для прогнозування ймовірності відтоку клієнтів телекомунікаційної компанії на основі їхніх даних. Використовуючи модель RandomForest, проєкт ідентифікує клієнтів із високим ризиком припинення співпраці та пропонує рекомендації для їх утримання. Інтерфейс Streamlit дозволяє завантажувати CSV-файли або вводити дані вручну, а результати відображаються з візуалізаціями (гістограми, індикатори). Проєкт упаковано в Docker для легкого розгортання.

## Основні можливості

1. Прогнозування ймовірності відтоку клієнтів за допомогою RandomForest.
2. Інтерактивний Streamlit-інтерфейс для завантаження CSV або ручного введення даних.
3. Візуалізація результатів: гістограми для кількох клієнтів, індикатори для одного.
4. Контейнеризація через Docker і Docker Compose.

## Структура проєкту

1. `datasets/internet_service_churn.csv` – Датасет для прогнозування.
2. `docs/` – Документація та зображення (скріншоти, діаграми).
3. `notebooks/eda.ipynb` – Попередній аналіз даних (EDA).
4. `src/` – Основний код:
   - `app.py` – Streamlit-додаток.
   - `inference.py` – Логіка прогнозування.
   - `model.py` – Навчання моделі.
   - `preprocessing.py` – Передобробка даних.
5. `Dockerfile` – Конфігурація Docker-образу.
6. `docker-compose.yml` – Налаштування Docker Compose.
7. `requirements.txt` – Список залежностей.
8. `README.md` – Опис проєкту.

## Вимоги

1. Python 3.10 (обов’язково для сумісності з Docker).
2. Docker і Docker Compose для контейнеризації.
3. Встановлені залежності з `requirements.txt`.
4. Датасет у `datasets/internet_service_churn.csv`.

## Як почати

1. Клонуйте репозиторій:
   ```bash
   git clone git@github.com:Oleksandr-Kyrychuk/DataScience_team_project.git
   ```
2. Створіть гілку:
   ```bash
   git checkout -b feature/ваша-назва
   ```
3. Переконайтеся, що датасет доступний у `datasets/internet_service_churn.csv`. Якщо ні,використайте шаблон CSV із Streamlit-додатка.
4. Встановіть залежності:
   ```bash
   pip install -r requirements.txt
   ```
5. Запустіть Streamlit-додаток:
   ```bash
   streamlit run src/app.py
   ```

## Налаштування оточення

1. Переконайтеся, що встановлено Python 3.10. Якщо ні, завантажте з [python.org](https://www.python.org/downloads/) і встановіть.
2. Створіть віртуальне оточення:
   ```bash
   python3.10 -m venv .venv
   ```
3. Активуйте оточення:
   ```bash
   source .venv/bin/activate  # Linux/Mac
   .venv\Scripts\activate     # Windows
   ```
4. Встановіть залежності:
   ```bash
   pip install -r requirements.txt
   ```
5. Налаштуйте `pre-commit` для автоматичних перевірок:
   ```bash
   pre-commit install
   ```

## Запуск із Docker

1. Переконайтеся, що Docker і Docker Compose встановлені.
2. Запустіть проєкт:
   ```bash
   docker-compose up --build
   ```
3. Відкрийте `http://localhost:8501` у браузері.
4. Зупиніть контейнер:
   ```bash
   docker-compose down
   ```

## Процес роботи

1. Створюйте нову гілку для кожної задачі:
   ```bash
   git checkout -b feature/назва-задачі
   ```
   Приклад:
   ```bash
   git checkout -b feature/eda-analysis
   ```
2. Додавайте змінені файли:
   ```bash
   git add .  # або конкретний файл
   ```
3. Робіть коміти:
   - Перед комітом запускайте:
     ```bash
     pre-commit run --all-files
     ```
     Це допомагає уникнути помилок (наприклад, `end-of-file-fixer`).
   - Виконуйте коміт:
     ```bash
     git commit -m "Опис змін"
     ```
4. Пуште зміни:
   ```bash
   git push origin feature/назва-задачі
   ```
5. Створюйте Pull Request (PR) на GitHub:
   - Назва: “Опис задачі” (наприклад, “Add EDA notebook”).
   - Опис: Короткий опис змін і призначення ревьювера.
6. Синхронізуйте локальну гілку `main`:
   ```bash
   git checkout main
   git pull origin main
   ```

## Використані бібліотеки

1. `pandas` – Обробка та аналіз даних.
2. `numpy` – Числові обчислення та операції з масивами.
3. `scikit-learn` – Машинне навчання (RandomForest).
4. `xgboost` – Градієнтний бустинг для прогнозування.
5. `lightgbm` – Оптимізований градієнтний бустинг.
6. `joblib` – Збереження та завантаження моделей.
7. `streamlit` – Інтерактивний веб-інтерфейс.
8. `matplotlib` – Візуалізація даних.
9. `seaborn` – Покращена візуалізація даних.
10. `plotly` – Інтерактивні графіки.

## Інструменти розробки

1. `jupyter` – Інтерактивні ноутбуки для аналізу даних.
2. `pre-commit` – Автоматичні перевірки коду перед комітом.
3. `black` – Форматування коду.
4. `flake8` – Перевірка стилю коду.
5. `nbqa` – Інтеграція інструментів перевірки для Jupyter ноутбуків.

## Приклад використання

1. Завантажте CSV-файл із даними клієнтів (шаблон доступний у Streamlit).
2. Отримайте прогноз із ймовірністю відтоку та рекомендаціями.
3. Перегляньте візуалізації (гістограма або індикатор).

Приклад результату:
```
Клієнт (ID: 1001): Висока ймовірність відтоку — 0.75
Рекомендація: Зв’яжіться з клієнтом для пропозиції знижок.
```

## Документація

1. Попередній аналіз (EDA): Див. `notebooks/eda.ipynb`.
2. Модель: RandomForestClassifier у `src/model.py`.
3. Інтерфейс: Streamlit-додаток у `src/app.py`.
4. Передобробка: Логіка в `src/preprocessing.py`.

